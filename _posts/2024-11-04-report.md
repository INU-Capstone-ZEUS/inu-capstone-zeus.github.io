---
title: 주간 보고서(10.28~11.03)
tags: TeXt
sidebar:
  nav: docs-ko
aside:
  toc: true
lang: ko
---


---

## 업무요약

<aside>
<img src="https://www.notion.so/icons/light-bulb_yellow.svg" alt="https://www.notion.so/icons/light-bulb_yellow.svg" width="40px" />

</aside>

- 박종현 : 학습 루프 제작 및 딥러닝 모델 성능 실험 진행
- 이제욱 :  S3, EC2 설정 및 FastAPI S3 업로드 테스트
- 조민수 :  키움증권 로그인 및 종목 데이터 받아오기 구현 및 32bit/64bit 환경 연결 시도
- 조하은 :  프론트 제작 및 크롤링 서버 배포(Docker), MockAPI를 통한 테스트 환경 구축


---


<!-- # 세부 업무 내용

# (종현)

## 라벨 분포 시간대 확인

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image.png?raw=true)

 클래스 불균형을 고려하여 반등과 하락 비율이 적절한 지점을 찾기 위하여 시간대에 따른 반등 빈도를 확인하였다. 9시~9시 30분 동안 특히 많은 급등이 포착되었으나, 이는 애초에 종목 선정을 9시 30분 이전에 급등한 종목만을 추출했기 때문으로 보인다. 따라서 9시 30분 이후에 반등이 주로 일어나는 시간대의 빈도를 확인하였을 때(그림 2), 시간이 지남에 따라 반등하는 경우가 적어짐을 확인할 수 있었다.

 따라서 반등 빈도가 현저히 적은 1시 이후를 학습 데이터에서 제외하고, 오전 9시 ~ 오후 1시까지 총 4시간의 데이터를 별도의 데이터셋(AM)으로 생성하여 기존 데이터셋(All)과 비교하여 학습을 진행하였다.

---

## 실험 환경 구성

### 공통 조건

- 최적화 : Adam(weigth decay 1e-4)
- 학습률 : 0.001
- Batch Size 32
- Epochs 100
- 학습률 스케줄링 미사용
- EarlyStopping 사용
- train 데이터셋 80%, 테스트 데이터셋 20% 사용
- Random Seed 42로 고정
- 조건식 A만 진행

### 1. 모델 구성

 실시간으로 주가를 입력받고 빠르게 매수 판단을 내려야하기 때문에 가능한 트랜스포머 모델을 제외하여 가벼운 모델부터 우선하여 실험하였다. 실험 모델은 아래와 같다.

- **LSTM-FCN 모델**

 RNN계열의 대표주자인 LSTM와 Linear레이어를 결합하여 만든 모델로, Paper With Code에서 Time Series Classification 분야에 상위 랭크에 존재하는 모델이다. 트랜스포머에 비해 비교적 가볍다는 이점이 분명하여 채택하였다. 다만 과적합을 고려하여 LSTM와 Linear 사이에 30% 비율의 Dropout을 적용하였다.

- **Dlinear 모델**

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%201.png?raw=true)

 Dlinear는 **Are Transformers Effective for Time Series Forecasting? (zeng et al., 2022)에서** 소개된 모델로, 간단한 1-Layer Linear Network를 사용하는 모델이다. 입력 시퀀스(Look bcak Window)를 시계열 분해를 통해 추세(Trend)와 나머지(Remainder)을 추출해내고, 이를 각각의 선형 네트워크에 입력하여 예측 결과를 합치는 것을 말한다. 정말 가볍고 간단한 모델이지만 LTSF(Long time Series Forecasting)에서 트랜스포머를 뛰어넘는 성능까지 증명되어 적용해보았다.

 추가적으로, Multivariate Input일 경우에 변수 간 Linear 레이어를 공유하는 Dlinear-S와, 변수별로 각기 다른 Linear 레이어를 가지는 Dlinear-I가 존재한다. 본 실험에서는 주가 뿐만 아니라 거래대금 및 다양한 기술지표들을 입력 변수로 삼기 때문에 Dlinear-S와 Dlinear-I 모두 실험을 진행하였다.

### 2. 데이터셋 구성

 두가지 데이터셋 종류로 나눠서 실험했으며, 각 데이터셋 모두 사이즈 10의 슬라이딩 윈도우를 적용한 윈도우 시퀀스로 구성되어 있다.

- **종일 데이터셋(All)**

 위에서 설명한대로 수집된 모든 시간대에 속하는 데이터를 입력 시퀀스로 변환한 데이터셋이다.

- **오전 데이터셋(AM)**

 마찬가지로 오후 1시까지의 데이터를 입력 시퀀스로 변환한 데이터셋이다.

### 3. Feature 구성

- 10 feature - 주가(시가, 종가,고가,저가), 거래대금 및 추가 기술지표를 종합한 10가지 지표
- 13 feature - 주가(시가, 종가,고가,저가), 거래대금 및 추가 기술지표를 종합한 13가지 지표

### 4. 손실 함수

- Cross Entropy Loss func
- Focal Loss func(Cross Entropy)

## 실험 결과

 위에서 설명한 모델 구조, 데이터셋 종류, Feature 구성, 손실 함수를 변경해가며 Training Loss와 Valid Loss의 변화(좌측), Valid Dateset의 Precision(True로 분류한 정확도)의 변화(우측)를 Plotting하여 기록하였다. 또한 학습 도중 에폭마다 Loss와 Precision을 텍스트 파일로 기록해두었다(하단 그림).

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%202.png?raw=true)

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%203.png?raw=true)

### 실험 결과 정리

 실험한 결과를 표로 정리하였다. 분류 모델이 반등 패턴으로 분류한 것의 정확도를 기재하였으며, 학습 과정중 최고 정확도(**Best Precision**)와 마지막 10에폭 동안의 평균 정확도(**Last 10 Average Precision**), 그리고 마지막 에폭의 정확도(**Final Precision**)를 기재하였다.

 실험 결과로 13개의 변수를 입력으로 사용한 LSTM-FCN이 All 데이터셋과 AM 데이터셋 모두에서 가장 높은 성능을 보여줬고, 특히 BCE 손실함수를 사용했을 때 가장 높은 정확도를 기록하였다.

|  |  |  | LSTM-FCN |  | Dlinear-S |  | Dlinear_I |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | **10 Features** | **13 Features** | **10 Features** | **13 Features** | **10 Features** | **13 Features** |
| **All Dataset** | **BCE** | **Best Precision** | 53.87% | **63.10%** | 10.40% | 13.06% | 10.48% | 16.18% |
|  |  | **Last 10 Average Precision** | 50.31% | **57.40%** | 7.24% | 10.85% | 6.65% | 9.15% |
|  |  | **Final Precision** | 48.08% | **59.74%** | 7.35% | 10.35% | 8.00% | 10.77% |
|  | **Focal BCE** | **Best Precision** | 53.48% | **55.23%** | 8.44% | 11.68% | 7.15% | 14.82% |
|  |  | **Last 10 Average Precision** | 45.64% | **43.05%** | 6.25% | 9.03% | 4.95% | 7.79% |
|  |  | **Final Precision** | 41.65% | **43.30%** | 6.85% | 8.28% | 5.55% | 9.12% |
| **AM Dataset** | **BCE** | **Best Precision** | 62.22% | **65.45%** | 14.52% | 20.94% | 16.23% | 25.42% |
|  |  | **Last 10 Average Precision** | 58.58% | **61.03%** | 11.60% | 17.04% | 12.03% | 18.08% |
|  |  | **Final Precision** | 61.02% | **61.66%** | 10.38% | 15.37% | 9.76% | 14.77% |
|  | **Focal BCE** | **Best Precision** | 55.27% | **57.87%** | 13.54% | 19.29% | 13.24% | 20.10% |
|  |  | **Last 10 Average Precision** | 49.57% | **49.07%** | 9.90% | 15.25% | 7.94% | 13.23% |
|  |  | **Final Precision** | 54.39% | **49.60%** | 10.10% | 12.56% | 5.26% | 12.35% |

### 추후 실험 계획

 성능 개선을 위해 가장 먼저 또 다른 모델을 대조군으로 실험할 계획이다. 또한 일봉 혹은 5분봉 데이터를 학습에 적용할 수 있는 방법을 연구하여 정확도를 향상시키는데 집중할 계획이다.

--- -->





<!-- # (제욱)

### Fastapi

현재 웹 서비스에서 Fastapi는 3개의 용도로 사용된다. 

- 모델 서빙

미리 학습된 모델을 fast api에 포함시켜 curl로 입력값을 보낼 경우 모델의 판별값을 받을 수 있게 한다. 우리의 서비스의 경우에는 시가, 거래대금 등이 입력이 되겠고 결과값으로 현재 주식 구매를 해야 할지 말지를 결과로 받게 된다. 

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%204.png?raw=true)

- 키움 증권 로그인 제공

키움 증권 api를 이용해 사용자가 서비스에 처음 접속했을 때 로그인할 수 있게끔 할 것이다.

- 웹 크롤링

검색기로 종목을 추출할 경우 웹 크롤링 url로 요청을 보내 gemini api 호출 결과값을 받아온다.

그 후에 aws s3에 저장한다. 저장된 분석 결과는 react에서 읽어온 뒤 사용자에게 보여준다. 

### S3 저장소 추가

지금까지 해왔던 것처럼 S3 또한 Terraform으로 생성하였다. 

```python
resource "aws_s3_bucket" "s3" {
  bucket = "${var.common_info.env}-${var.common_info.service_name}-bucket"

  tags = {
    environment = "${var.common_tags.Environment}"
  }
}

resource "aws_s3_bucket_public_access_block" "public-access" {
  bucket = aws_s3_bucket.s3.id

  block_public_acls       = false
  block_public_policy     = false
  ignore_public_acls      = false
  restrict_public_buckets = false
}

resource "aws_s3_bucket_policy" "bucket-policy" {
  bucket = aws_s3_bucket.s3.id

  depends_on = [
    aws_s3_bucket_public_access_block.public-access
  ]

  policy = <<POLICY
{
  "Version":"2012-10-17",
  "Statement":[
    {
      "Sid":"PublicRead",
      "Effect":"Allow",
      "Principal": "*",
      "Action":["s3:GetObject"],
      "Resource":["arn:aws:s3:::${aws_s3_bucket.s3.id}/*"]
    }
  ]
}
POLICY
}
```

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%205.png?raw=true)

또 React에서 S3를 호출하기 위해 cors 설정을 추가한다.

```python
resource "aws_s3_bucket_cors_configuration" "example" {
  bucket = aws_s3_bucket.s3.id

  cors_rule {
      allowed_headers = ["*"]
      allowed_methods = ["GET", "POST", "PUT"]
      allowed_origins = ["*"]
      expose_headers  = ["ETag"]
      max_age_seconds = 3000
  }
}
```

### EC2 인스턴스 역할 추가

서비스 컨테이너들이(React, Fastapi)는 S3에 접근하기 위해서는 권한이 있어야 한다. AWS IAM USER를 생성한 뒤 컨테이너 내부에서 직접 연결하는 방식도 있겠지만 보안적으로 문제가 생길 수 있기 때문에 키 값을 HardCoding 할 수는 없고 위험을 피하기 위해 환경변수로 설정하는 것은 귀찮은 과정이다.

이 때 주로 사용되는 방법이 EC2 인스턴스 자체에 S3를 사용할 권한을 부여하는 것이다. 이렇게 함으로서 EC2 아래에서 띄워지는 컨테이너들은 S3에 접근할 권한을 자동적으로 취득한다. 

역할을 생성한 뒤 권한을 역할에 연결하고 ec2에 그 역할을 할당하는 것으로 과정이 완료된다.

해당 과정 또한 Terraform으로 생성한다.

```python
resource "aws_iam_policy" "s3_access_policy" {
  name        = "ec2_s3_access_policy"
  description = "Allow EC2 to access S3 buckets"

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect   = "Allow",
        Action   = [
          "s3:ListBucket",
          "s3:GetObject",
          "s3:PutObject"
        ],
        Resource = [
          "arn:aws:s3:::dev-jeus-bucket",          # S3 버킷
          "arn:aws:s3:::dev-jeus-bucket/*"         # 버킷 내 객체
        ]
      }
    ]
  })
}

# 역할에 정책 연결
resource "aws_iam_role_policy_attachment" "ec2_role_policy_attach" {
  role       = aws_iam_role.ec2_role.name
  policy_arn = aws_iam_policy.s3_access_policy.arn
}

resource "aws_iam_role" "ec2_role" {
  name = "ec2_s3_access_role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Principal = {
          Service = "ec2.amazonaws.com"
        },
        Action = "sts:AssumeRole"
      }
    ]
  })
}

# IAM 인스턴스 프로파일 생성
resource "aws_iam_instance_profile" "ec2_instance_profile" {
  name = "ec2_instance_profile"
  role = aws_iam_role.ec2_role.name
}
```

해당 부분을 인스턴스 생성한 파일에 추가한다.

```python
iam_instance_profile = aws_iam_instance_profile.ec2_instance_profile.name
```

역할이 권한과 잘 연결되고 또 ec2 인스턴스에 역할이 할당된 것을 확인가능하다.

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%206.png?raw=true)

### Fastapi S3 업로드

/crawl-and-analyze router에 다음 코드를 추가해서 gemini api 사용 결과를 s3에 업로드하게끔 한다.

```python
import boto3

# S3 클라이언트 생성
s3_client = boto3.client("s3")

# JSON 데이터 S3 업로드
    s3_bucket_name = "dev-jeus-bucket"  
    file_name = f"{company_name}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    json_data = json.dumps([article.dict() for article in analysis_dto], ensure_ascii=False, indent=4)

    try:
        s3_client.put_object(
            Bucket=s3_bucket_name,
            Key=file_name,
            Body=json_data,
            ContentType='application/json'
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to upload file to S3: {e}")
```

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%207.png?raw=true)

### 추후 계획

만약 처음 페이지를 켰을 때 s3에 아무 것도 없으면 아무 칸이 보이질 않는다.

사용자가 계속해서 내용이 보일 때까지 새로고침을 한다면 접근성이 떨어지는 웹 서비스가 될 것이다. 따라서 웹 서비스에서 주기적으로 업로드 되는 것을 알고 자동으로 변경사항을 사용자에게 제공해야한다.

따라서 서비스에 S3 -> lambda -> websocket api로 이어지는 실시간 처리를 도입하는 것을 목표로 한다. s3에 객체가 업로드 되면 람다 함수가 트리거 되고 websocket api로 react에 전달한다. react에선 알림을 받고 내용을 불러와 사용자에게 바로 보여준다.

이렇게 함으로서 사용자는 실시간으로 데이터가 업데이트되는 것을 알 수 있다.

---





# (민수)

## 키움 증권 api

키움증권 open api를 사용해서 로그인 기능을 구현하였다.

우선 키움증권 api를 사용하기위해 python을 가상환경에서 32bit으로 구축하였다.

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/32%25E3%2585%25A0%25E3%2585%2591%25E3%2585%2585.png?raw=true)

키움증권 api를 연결하기 위해 pykiwoom 패키지를 설치하고

```jsx
kiwoom = Kiwoom()
kiwoom.CommConnect(block=True)
```

다음과 같은 코드를 작성하여 로그인창이 표시되도록 구현하였다.

추가적으로 기존 구현된 종목데이터를 받아와서 차트를 구현하였다. 일단 웹사이트에 보여주기 전 그려지는 모습을 확인하고자 matplot을 통해 캔들차트 형식으로 구현하였다. 대표적인 주식인 삼성전자에 대한 데이터를 기반으로 구현해보았다.

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/Figure_1.png?raw=true)

다음과 같은 형식으로 일봉데이터에 대해서 차트가 그려지는 것을 확인 할 수있었다.

```jsx
import pandas as pd
    df = pd.DataFrame(kiwoom.ohlcv, columns=['date', 'open', 'high', 'low', 'close', 'volume'])
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')

    # 차트 그리기
    fig, ax = plt.subplots(figsize=(12, 6))
    candlestick_ohlc(ax, zip(mdates.date2num(df.index),
                             df['open'], df['high'],
                             df['low'], df['close']),
                     width=0.6, colorup='r', colordown='b')
```

## 문제점

현재 구현된 로그인과 종목데이터 받아오기 및 차트에 대한 코드들은 키움증권 api에서 받아오기 위해서 32bit환경에서 작성되어진 코드이다.

구현된 코드들을 기반으로 fast api에 추가하여 코드를 작성을 시도해 보았으나 현재 작성되어진 fastapi의 경우 64bit 환경에 구현이 되어있어 버전이 맞지 않아 에러가 나오는 현상을 발견하였다.

이를 해결하기 위해 서버를 하나 더 구축하여 fastapi사용하는 서버와 키움증권 api를 사용하는 서버를 사용하여 서버간의 통신을 통하여 구축할 수 있을것이라고 생각하였다.

일단 글로벌 서버를 구축하지 않고 로컬 서버에서 위에 해당하는 방법이 가능한지 테스트진행을 해보았다.

```python
# 32bit_kiwoom_server.py (32비트 환경에서 실행)
from flask import Flask, jsonify
from pykiwoom.kiwoom import *
import pythoncom

app = Flask(__name__)
kiwoom = Kiwoom()

@app.route('/login', methods=['POST'])
def login():
    pythoncom.CoInitialize()  # COM 객체 초기화
    
    # 로그인 창 표시
    kiwoom.CommConnect(block=True)
    
    if kiwoom.GetConnectState() == 1:
        login_info = {
            "status": "logged_in",
            "message": "Login successful"
        }
    else:
        login_info = {
            "status": "login_failed",
            "message": "Login failed"
        }
    
    pythoncom.CoUninitialize()  # COM 객체 해제
    
    return jsonify(login_info)

@app.route('/user_info')
def user_info():
    if kiwoom.GetConnectState() == 0:
        return jsonify({"error": "Not connected"}), 400
    
    account_num = kiwoom.GetLoginInfo("ACCOUNT_CNT")
    accounts = kiwoom.GetLoginInfo("ACCNO")
    user_id = kiwoom.GetLoginInfo("USER_ID")
    user_name = kiwoom.GetLoginInfo("USER_NAME")
    return jsonify({
        "account_num": account_num,
        "accounts": accounts,
        "user_id": user_id,
        "user_name": user_name
    })

if __name__ == '__main__':
    app.run(host='127.0.0.1', port=5000)
```

```python
#64bit fastapi
from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.responses import JSONResponse
import httpx

app = FastAPI()
router = APIRouter()

KIWOOM_API_URL = "http://127.0.0.1:5000"

@router.get("/login")
@router.post("/login")
async def kiwoom_login():
    async with httpx.AsyncClient() as client:
        response = await client.post(f"{KIWOOM_API_URL}/login")
    if response.status_code == 200:
        return JSONResponse(content=response.json())
    else:
        raise HTTPException(status_code=response.status_code, detail="Login failed")

@router.get("/user_info")
async def kiwoom_user_info():
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{KIWOOM_API_URL}/user_info")
    if response.status_code == 200:
        return JSONResponse(content=response.json())
    elif response.status_code == 400:
        raise HTTPException(status_code=400, detail="Not connected to Kiwoom API")
    else:
        raise HTTPException(status_code=response.status_code, detail="Failed to get user info from Kiwoom API")

app.include_router(router, prefix="/kiwoom")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

위의 32bit에서 구현한 파일을 실행하게되면 다음과 같이 32bit에서 로컬 서버가 하나 구축이 된다. 

추가적으로 64bit에서 구현한 파일을 실행하면

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%208.png?raw=true)

다음과 같은 결과가 나타나고 주소를 입력하면 다음과 같이 키움증권 api로그인 화면이 구현되었다.

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/1234.png?raw=true)

---






# (하은)

## 도커를 통한 크롤링 서버 배포

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%209.png?raw=true)

FastAPI를 사용하여 크롤링 서버를 구축하였다.

기존에 터미널에서 직접 실행시켰던 크롤링 파일과 뉴스 분석 파일을 하나의 라우터에서 post 요청이 오면 실행시키도록 코드를 수정하였다.

또한 Docker image로 빌드하여 Docker Hub에 올린뒤  ec2에서 pull 받아 실행시킴으로써 배포도 진행하였다.

## 크롤링 서버 코드

```jsx
from bs4 import BeautifulSoup
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import datetime
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from pydantic import BaseModel
from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse
from typing import List, Dict, Any

# 크롤링 요청 데이터 모델
class CrawlRequest(BaseModel):
    company_code: str
    page: int

# 라우터 설정
router = APIRouter()

class CrawlError(Exception):
    """Custom exception for errors during crawling."""
    def __init__(self, message: str):
        self.message = message
        super().__init__(self.message)

# Selenium 설정 함수
def get_browser():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=chrome_options)
    return driver

class CrawlError(Exception):

    def __init__(self, message: str):
        self.message = message
        super().__init__(self.message)

def crawl_news(company_code: str, page: int) -> List[Dict[str, str]]:
    driver = None  # driver 초기화
    try:
        if(len(company_code) != 6):
            print("종목 코드가 올바른 형식이 아닙니다.")
            raise CrawlError("종목 코드가 올바른 형식이 아닙니다.")
        
        if( page < 1):
            print("페이지 번호는 1과 200 사이여야 합니다.")
            raise CrawlError("페이지 번호는 1과 200 사이여야 합니다.")    

        
        # URL / 요청 헤더 설정
        url = f'https://finance.naver.com/item/news.naver?code={company_code}&page={page}'
        driver = get_browser()
        driver.get(url)

        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'news_frame')))
        driver.switch_to.frame('news_frame')

        source_code = driver.page_source
        html = BeautifulSoup(source_code, "html.parser")

        # 중복 뉴스 제거
        for tr in html.select('tr.relation_lst'):
            tr.decompose()

        # 기사 item의 신문사 / 날짜 / 뉴스 주소 갖고 오기
        infos = html.select('.info')
        dates = html.select('.date')
        aTags = html.select('td.title a')

        links = [a.attrs['href'] for a in aTags]
        articles = []

        for i, full_url in enumerate(links):
            try:
                driver.get(full_url)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'article')))
            except Exception as e:
                raise CrawlError(f"기사 페이지 로딩 실패: {full_url}")

            new_page_source = driver.page_source
            soup = BeautifulSoup(new_page_source, 'html.parser')

            for div in soup.select('div.vod_player_wrap._VIDEO_AREA_WRAP'):
                div.decompose()

            for div in soup.select('div.artical-btm'):
                div.decompose()

            for br in soup.find_all("br"):
                br.replace_with("\n")

            article_content = soup.select_one('article').text.strip()
            article_title = soup.select_one('#title_area span').text.strip()

            article = {
                'title': article_title,
                'publisher': infos[i].text.strip() if i < len(infos) else 'Unknown',
                'date': dates[i].text.strip() if i < len(dates) else 'Unknown',
                'link': full_url,
                'content': article_content,
            }

            articles.append(article)

        driver.quit()

    # 모든 크롤링 작업이 끝난 후 브라우저 종료
    except CrawlError as e:
        if driver:
            driver.quit()
        raise e  # CrawlError를 다시 발생시켜서 상위 함수에서 처리
    except Exception as e:
        if driver:
            driver.quit()
        raise CrawlError(f"크롤링 중 에러 발생: {str(e)}")  # 모든 에러를 CrawlError로 감싸서 던짐

    return articles

```

```jsx
import json
from datetime import datetime
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List
from .crawl_news import crawl_news, CrawlError 
from .analyze_news import analyze_news

router = APIRouter()

# Gemini 분석 요청 
class AnalysisReqDTO(BaseModel):
    evaluation: str
    summary: str

# Gemini 분석 응답
class AnalysisResDTO(BaseModel):
    evaluation: str
    summary: str
    link: str
    title: str

# 전체 요청
class CrawlAndAnalyzeRequest(BaseModel):
    company_code: str
    page: int
    company_name: str

# 전체 응답
class CrawlAndAnalyzeResponse(BaseModel):
    status: str
    total_articles: int 
    analysis: List[AnalysisResDTO]

@router.post("/crawl-and-analyze", response_model=CrawlAndAnalyzeResponse)
async def crawl_and_analyze(request: CrawlAndAnalyzeRequest) -> CrawlAndAnalyzeResponse:
    company_code = request.company_code
    page = request.page
    company_name = request.company_name

    # 크롤링 단계
    try:
        articles = crawl_news(company_code, page)
    except CrawlError as e:  # CrawlError만 처리
        raise HTTPException(status_code=400, detail=f"Crawling error: {e.message}")

    except Exception as e:
        # 예기치 않은 에러 처리
        raise HTTPException(status_code=400, detail=f"Crawling error: {e.message}")

    # 분석 단계
    try:
        analyzed_articles = analyze_news(articles, company_name)
    except Exception as e:
        # Gemini API 요청 제한으로 인해 발생하는 에러 처리
        raise HTTPException(status_code=400, detail=f"Gemini API error: {e.message}")

    # 분석 결과를 DTO로 변환
    analysis_dto = [AnalysisResDTO(**article) for article in analyzed_articles]

    # 응답 데이터를 JSON 파일로 저장
    file_name = f"{company_name}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(file_name, "w", encoding="utf-8") as json_file:
        json.dump([article.dict() for article in analysis_dto], json_file, ensure_ascii=False, indent=4)

    # 응답 데이터 생성
    return CrawlAndAnalyzeResponse(
        status="success",
        total_articles=len(analysis_dto),
        analysis=analysis_dto
    )

```

```jsx
import os
import google.generativeai as genai
from dotenv import load_dotenv
import typing_extensions as typing
from fastapi import APIRouter
from typing import Dict, Any, List
import json
from fastapi import APIRouter, HTTPException

class Analysis(typing.TypedDict):
    evaluation: str
    reason: str
    summary: str

router = APIRouter()

def configure_gemini_api():
    load_dotenv()
    GOOGLE_API_KEY = os.getenv('MY_KEY')
    genai.configure(api_key=GOOGLE_API_KEY)

def analyze_article(article_content: str, company_name: str, article_link: str, article_title: str):

    if not article_content:
        return {
            "evaluation": "Error",
            "summary": "분석 실패",
            "link": article_link,  # 링크는 결과에만 포함,
            "title": article_title
        }
    
    prompt = f"""
    다음의 Article을 바탕으로, 이 기사가 ${company_name} 종목에 대해 긍정적인 평가를 내리고 있는지, 부정적인 평가를 내리고 있는지, 또는 종목과 관련이 없는지 판단해줘. [긍정 / 관련 없음 / 부정] 중 하나의 단어로 평가를 내려서 evaluation 필드에 저장해줘. 또한 이 기사에 포함된 종목 관련 정말 중요한 기사의 핵심 내용을 한 번에 알아볼 수 있도록 한 줄로 간략하게 요약해서 'summary'에 저장해줘. 반환 결과는 반드시 JSON 형식이어야 해.
    여기서 'summary' 필드 예시를 들어보자면 다음과 같아.
    
    Article Example:
    {{'evaluation': '긍정, 'summary': '${company_name}가 3분기 실적에서 견조한 매출을 기록 및 '헬시 플레저' 트렌드에 부합하는 제품 출시 및 좋은 성적 도출'}}

    Article:
    {article_content}

    반환 형식은 아래와 같아:
    Analysis = {{'evaluation': str, 'summary': str}}
    Return: Analysis
    """
    
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        result = model.generate_content(prompt)

        # candidates가 비어있는지 먼저 확인
        if not result.candidates or not result.candidates[0].content.parts:
            print(prompt);
            print(result);
            raise ValueError("API 응답이 비어 있습니다.")
        
        # API에서 반환된 텍스트 정리
        result_text = result.candidates[0].content.parts[0].text
        cleaned_response = result_text.replace("```json", "").replace("```", "").strip()
        
        # JSON 형식으로 변환
        analysis_result = json.loads(cleaned_response)
        analysis_result['link'] = article_link  # 링크는 결과에만 포함
        analysis_result['title'] = article_title  # 링크는 결과에만 포함
        
        return analysis_result  # 분석 결과 반환
    
    except Exception as e:
        print(f"Error during analysis: {e}")
        return {
            "evaluation": "Error",
            "summary": "API 요청에 실패했습니다.",
            "link": article_link,  # 실패 시에도 링크를 반환
            "title": article_title
        }

# 뉴스 기사 분석 함수
def analyze_news(articles: List[Dict[str, str]], company_name: str) -> List[Dict[str, str]]:
    try:
        configure_gemini_api()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"API 설정 오류: {str(e)}")
    
    analyzed_articles = []

    for article in articles:
        # 각 기사에 'content' 필드가 없거나 비어 있을 경우 처리
        if 'content' not in article or not article['content']:
            analyzed_articles.append({
                "evaluation": "Error",
                "summary": "기사 내용이 없습니다.",
                "link": article.get('link', 'unknown'),  # 링크 필드 추가
                "title": article.get('title', 'unknown')  # 링크 필드 추가
            })
            continue
        
        # 각 기사를 분석하여 결과를 추가
        analysis = analyze_article(article['content'], company_name, article['link'], article['title'])
        analyzed_articles.append(analysis)

    return analyzed_articles
```

## 프론트 수정

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%2010.png?raw=true)

필요한 컴포넌트만 포함시켜 한 페이지 내에 다 보이도록 코드를 수정하였다.

또한 모달 컴포넌트도 추가로 작성하였다. 서비스를 사용하기 위해서는 영웅문에 로그인해야하기 때문에 recoil을 사용하여 사용자의 로그인 여부를 판단하고 모달을 띄우도록 코드를 작성하였다.

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%2011.png?raw=true)

## MockAPI를 사용한 테스트 환경 구축

![image.png](https://github.com/INU-Capstone-ZEUS/inu-capstone-zeus.github.io/blob/master/assets/images/report/_9_report/image%2012.png?raw=true)

로컬에서 실행시키면 위와같은 정보가 추가적으로 보인다. 백엔드가 개발되기 전에도 서버가 개발된 것처럼 진행할 수 있도록 mockAPI를 사용하다. -->



---

If you like TeXt, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/kitian616/jekyll-TeXt-theme.svg?label=Stars&style=social)](https://github.com/kitian616/jekyll-TeXt-theme/)
